2023-04-09 14:18:32,030 - INFO - Config:
2023-04-09 14:18:32,030 - INFO - {
    "L2_regularisation": 0,
    "alpha": 100,
    "base_dir": "models/experiments/eICU/LoS/TPC",
    "batch_size": 32,
    "batch_size_test": 32,
    "batchnorm": "mybatchnorm",
    "dataset": "eICU",
    "diagnosis_size": 64,
    "disable_cuda": false,
    "exp_name": "TPC",
    "intermediate_reporting": false,
    "kernel_size": 4,
    "labs_only": false,
    "last_linear_size": 17,
    "learning_rate": 0.00226,
    "loss": "msle",
    "main_dropout_rate": 0.45,
    "mode": "train",
    "model_type": "tpc",
    "n_epochs": 5,
    "n_layers": 9,
    "name": "TPC",
    "no_diag": false,
    "no_exp": false,
    "no_labs": false,
    "no_mask": false,
    "no_skip_connections": false,
    "no_temp_kernels": 12,
    "percentage_data": 100.0,
    "point_size": 13,
    "point_sizes": [
        13,
        13,
        13,
        13,
        13,
        13,
        13,
        13,
        13
    ],
    "save_results_csv": false,
    "seed": 1540177368,
    "share_weights": false,
    "shuffle_train": false,
    "sum_losses": true,
    "task": "LoS",
    "temp_dropout_rate": 0.05,
    "temp_kernels": [
        12,
        12,
        12,
        12,
        12,
        12,
        12,
        12,
        12
    ]
}
2023-04-09 14:18:34,633 - INFO - Experiment set up.
2023-04-09 14:18:34,735 - INFO - TempPointConv(
  (relu): ReLU()
  (sigmoid): Sigmoid()
  (hardtanh): Hardtanh(min_val=0.020833333333333332, max_val=100)
  (msle_loss): MSLELoss(
    (squared_error): MSELoss()
  )
  (mse_loss): MSELoss(
    (squared_error): MSELoss()
  )
  (bce_loss): BCELoss()
  (main_dropout): Dropout(p=0.45, inplace=False)
  (temp_dropout): Dropout(p=0.05, inplace=False)
  (empty_module): EmptyModule()
  (diagnosis_encoder): Linear(in_features=293, out_features=64, bias=True)
  (bn_diagnosis_encoder): MyBatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn_point_last_los): MyBatchNorm1d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (bn_point_last_mort): MyBatchNorm1d(17, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (point_final_los): Linear(in_features=17, out_features=1, bias=True)
  (point_final_mort): Linear(in_features=17, out_features=1, bias=True)
  (layer_modules): ModuleDict(
    (0): ModuleDict(
      (temp): Conv1d(174, 1044, kernel_size=(4,), stride=(1,), groups=87)
      (bn_temp): MyBatchNorm1d(1044, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (point): Linear(in_features=241, out_features=13, bias=True)
      (bn_point): MyBatchNorm1d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (1): ModuleDict(
      (temp): Conv1d(1300, 1200, kernel_size=(4,), stride=(1,), dilation=(3,), groups=100)
      (bn_temp): MyBatchNorm1d(1200, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (point): Linear(in_features=1298, out_features=13, bias=True)
      (bn_point): MyBatchNorm1d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (2): ModuleDict(
      (temp): Conv1d(1469, 1356, kernel_size=(4,), stride=(1,), dilation=(6,), groups=113)
      (bn_temp): MyBatchNorm1d(1356, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (point): Linear(in_features=1454, out_features=13, bias=True)
      (bn_point): MyBatchNorm1d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (3): ModuleDict(
      (temp): Conv1d(1638, 1512, kernel_size=(4,), stride=(1,), dilation=(9,), groups=126)
      (bn_temp): MyBatchNorm1d(1512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (point): Linear(in_features=1610, out_features=13, bias=True)
      (bn_point): MyBatchNorm1d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (4): ModuleDict(
      (temp): Conv1d(1807, 1668, kernel_size=(4,), stride=(1,), dilation=(12,), groups=139)
      (bn_temp): MyBatchNorm1d(1668, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (point): Linear(in_features=1766, out_features=13, bias=True)
      (bn_point): MyBatchNorm1d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (5): ModuleDict(
      (temp): Conv1d(1976, 1824, kernel_size=(4,), stride=(1,), dilation=(15,), groups=152)
      (bn_temp): MyBatchNorm1d(1824, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (point): Linear(in_features=1922, out_features=13, bias=True)
      (bn_point): MyBatchNorm1d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (6): ModuleDict(
      (temp): Conv1d(2145, 1980, kernel_size=(4,), stride=(1,), dilation=(18,), groups=165)
      (bn_temp): MyBatchNorm1d(1980, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (point): Linear(in_features=2078, out_features=13, bias=True)
      (bn_point): MyBatchNorm1d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (7): ModuleDict(
      (temp): Conv1d(2314, 2136, kernel_size=(4,), stride=(1,), dilation=(21,), groups=178)
      (bn_temp): MyBatchNorm1d(2136, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (point): Linear(in_features=2234, out_features=13, bias=True)
      (bn_point): MyBatchNorm1d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
    (8): ModuleDict(
      (temp): Conv1d(2483, 2292, kernel_size=(4,), stride=(1,), dilation=(24,), groups=191)
      (bn_temp): MyBatchNorm1d(2292, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (point): Linear(in_features=2390, out_features=13, bias=True)
      (bn_point): MyBatchNorm1d(13, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
    )
  )
  (point_last_los): Linear(in_features=2781, out_features=17, bias=True)
  (point_last_mort): Linear(in_features=2781, out_features=17, bias=True)
)
2023-04-09 14:39:28,177 - INFO - Custom bins confusion matrix:
2023-04-09 14:39:28,320 - INFO - [[1234277  651549  136821   42897   17303    8007    4250    2455    4287
     1157]
 [ 421464  516490  176129   64267   27143   13056    7152    4197    6985
     1568]
 [ 138598  300648  168456   73762   33633   17134    9598    5802   10001
     2361]
 [  54405  164455  130394   69480   35013   18971   11064    6656   12609
     2983]
 [  26511   95608   92380   57281   32178   18737   11508    7255   14081
     3737]
 [  15038   61165   64293   45218   28160   17071   11040    7150   14718
     4202]
 [   9153   41160   46955   35303   23603   15130    9859    6836   14761
     4680]
 [   5754   28497   34627   28179   19624   13109    8953    6120   13792
     5086]
 [  13327   73376   98859   85271   64187   45107   32229   23629   59551
    26290]
 [   7219   38563   55308   51778   41577   31316   23722   17547   46393
    23405]]
2023-04-09 14:39:33,292 - INFO - Epoch: 0 | Train Loss: 83.5763
2023-04-09 14:43:46,362 - INFO - Custom bins confusion matrix:
2023-04-09 14:43:46,362 - INFO - [[300121 114907  23274   7140   2570   1188    459    192     67     24]
 [ 69303 121813  45887  15937   5913   2522   1041    523    265     38]
 [ 10972  55025  50798  25014  10634   4925   2247   1033    720     24]
 [  3215  19428  31751  24439  13795   7295   3592   1554   1695     23]
 [  1236   8442  16624  17945  13151   8514   4712   2431   2434     53]
 [   846   4850   9131  12109  10866   7861   5164   2719   3463    139]
 [   482   2643   5099   7724   8246   6863   4895   3199   4104    205]
 [   360   1695   3619   4967   5534   5676   4439   3117   4998    220]
 [   868   4099   7320  11332  14584  16531  14285  11602  26978   3597]
 [   469   1701   2873   5060   8210   9705   9733   8821  21230   3886]]
2023-04-09 14:43:48,827 - INFO - Epoch: 0 | Validation Loss: 53.1783
2023-04-09 15:04:59,228 - INFO - Custom bins confusion matrix:
2023-04-09 15:04:59,229 - INFO - [[1451304  503218   99115   29085   10528    4485    2129    1134    1752
      253]
 [ 381491  537097  214051   60520   22609   10010    4953    2875    4173
      672]
 [  77109  249376  256185   96238   38912   18019    9427    5226    8099
     1402]
 [  22410   94126  172652  104580   49821   25271   13683    8049   13109
     2329]
 [   9669   40849   97719   84545   50107   27990   16676    9967   18116
     3638]
 [   5367   21723   55839   61391   42643   26407   16819   10958   21715
     5193]
 [   3150   12455   35188   43493   33714   22832   15793   10863   23319
     6633]
 [   1988    7346   23222   30946   26452   19183   13812    9853   23199
     7740]
 [   4890   17639   53768   80494   74743   60269   46139   35650   98621
    49613]
 [   2829    8197   25870   42176   42917   37402   30737   24709   75034
    46957]]
2023-04-09 15:05:04,128 - INFO - Epoch: 1 | Train Loss: 58.0747
2023-04-09 15:09:17,767 - INFO - Custom bins confusion matrix:
2023-04-09 15:09:17,767 - INFO - [[344997  83010  13648   4996   1952    833    264    178     62      2]
 [ 69243 127376  44885  13041   4735   2224    952    432    349      5]
 [  9046  51623  55432  25688  10647   4928   2160    900    966      2]
 [  2868  17403  28419  25114  16123   8487   4298   2049   1965     61]
 [  1177   7018  12654  16360  14456  10488   6136   3345   3797    111]
 [   755   3863   6460   9555  10004   9178   6789   4369   5895    280]
 [   369   2335   3455   5238   6781   6952   6092   4397   7377    464]
 [   356   1354   2261   3444   4547   4911   4819   3868   8329    736]
 [   787   3143   5024   8154  10272  11840  12868  11717  38015   9376]
 [   508   1338   2143   3347   4788   6224   7372   7868  27761  10339]]
2023-04-09 15:09:19,343 - INFO - Epoch: 1 | Validation Loss: 40.9249
2023-04-09 15:30:25,334 - INFO - Custom bins confusion matrix:
2023-04-09 15:30:25,335 - INFO - [[1525881  458933   80972   21959    7928    3325    1659     889    1294
      163]
 [ 338035  577076  234511   52587   18566    7961    4011    2095    3115
      494]
 [  54654  231218  304651   97466   36228   15873    7901    4287    6622
     1093]
 [  15476   75910  188920  114567   51960   24974   13077    7460   11574
     2112]
 [   6995   30903   95683   93026   53790   30107   17369   10289   17659
     3455]
 [   3975   15892   50377   63311   46008   29506   18692   12048   23038
     5208]
 [   2311    8921   29964   42748   35548   25404   17350   11886   26211
     7097]
 [   1446    5159   18927   29719   26706   20433   14984   10895   26533
     8939]
 [   3629   11794   40614   71712   69902   61040   48343   38393  114189
    62210]
 [   2045    5235   18658   36414   37172   36210   30932   25531   83862
    60769]]
2023-04-09 15:30:30,235 - INFO - Epoch: 2 | Train Loss: 49.7458
2023-04-09 15:34:43,697 - INFO - Custom bins confusion matrix:
2023-04-09 15:34:43,698 - INFO - [[345036  86675  12554   3515   1213    508    203    108    130      0]
 [ 52144 144442  50010   9992   3736   1546    754    298    320      0]
 [  5005  44345  72135  23666   9032   3793   1720    767    918     11]
 [  1432  12762  35890  26912  15162   7077   3738   1892   1884     38]
 [   563   5053  14360  17488  15303  10111   5491   3438   3566    169]
 [   404   2788   6919   9569  10380   9602   6664   4455   5975    392]
 [   194   1555   3155   5276   6706   7124   6370   4471   7958    651]
 [   236    821   1866   3248   4338   5132   4671   4233   9313    767]
 [   471   2223   4035   6257   8496  10601  11492  11841  44629  11151]
 [   259    878   1608   2445   3656   4610   5582   6161  33421  13068]]
2023-04-09 15:34:45,161 - INFO - Epoch: 2 | Validation Loss: 34.6989
2023-04-09 15:55:53,954 - INFO - Custom bins confusion matrix:
2023-04-09 15:55:53,954 - INFO - [[1572282  432044   68446   18196    6437    2737    1213     653     864
      131]
 [ 314601  603166  242825   47140   16164    6833    3250    1727    2415
      330]
 [  42786  218906  336507   95876   33730   14497    7105    3933    5749
      904]
 [  12101   64928  198689  120881   52823   24288   12701    7012   10711
     1896]
 [   5390   24930   93922   97504   57454   31215   17669   10364   17653
     3175]
 [   3011   12521   46298   65092   48645   31169   19745   12539   23753
     5282]
 [   1656    6734   26232   42381   36901   26561   18457   12931   27986
     7601]
 [   1096    3920   16239   27857   26710   21410   15823   11721   29075
     9890]
 [   2742    8814   33267   65656   67010   61617   49486   39788  123420
    70026]
 [   1403    3775   14454   32373   33837   35233   30628   25939   89339
    69847]]
2023-04-09 15:55:59,015 - INFO - Epoch: 3 | Train Loss: 44.6264
2023-04-09 16:00:13,022 - INFO - Custom bins confusion matrix:
2023-04-09 16:00:13,023 - INFO - [[362864  73847   8567   2734   1040    499    253     66     72      0]
 [ 55268 150859  43058   8425   3149   1394    617    290    182      0]
 [  5282  48041  70382  22460   8470   3751   1703    765    516     22]
 [  1595  13412  35244  26819  14978   7371   3794   1879   1659     36]
 [   660   5093  13883  16695  15457  10120   6110   3602   3756    166]
 [   389   2616   6304   9162  10370   9755   6907   4709   6511    425]
 [   245   1399   2991   4503   6368   7478   6338   4883   8632    623]
 [   239    858   1724   2740   3876   4722   5066   4271  10281    848]
 [   503   2072   3342   5467   7448   9593  11388  11327  47889  12167]
 [   196    871   1295   2114   3191   4421   5383   6248  32924  15045]]
2023-04-09 16:00:14,817 - INFO - Epoch: 3 | Validation Loss: 30.9520
2023-04-09 16:21:23,313 - INFO - Custom bins confusion matrix:
2023-04-09 16:21:23,313 - INFO - [[1603956  413904   59506   15181    5422    2362    1120     597     847
      108]
 [ 291776  630179  246157   42650   14706    6060    2861    1540    2203
      319]
 [  34533  210169  357510   95633   32589   13695    6454    3479    5149
      782]
 [   9357   56347  205188  125669   54232   24542   12340    6627   10076
     1652]
 [   4080   20727   90954  101748   60140   32656   18239   10395   17361
     2976]
 [   2328   10370   43852   65250   50664   32484   20402   13019   24540
     5146]
 [   1335    5446   23566   41531   38147   27754   19562   13615   28979
     7505]
 [    827    3057   14241   27041   27386   22010   16609   12460   30254
     9856]
 [   2099    6790   28887   62395   65150   61758   50719   41357  128142
    74529]
 [   1069    3163   12467   30041   30653   33591   29738   25508   92873
    77725]]
2023-04-09 16:21:28,221 - INFO - Epoch: 4 | Train Loss: 40.9867
2023-04-09 16:25:41,703 - INFO - Custom bins confusion matrix:
2023-04-09 16:25:41,704 - INFO - [[369676  67874   7927   2576    936    474    223    150     98      8]
 [ 46793 160653  42329   7445   3114   1501    719    345    337      6]
 [  3608  45593  77515  19726   7408   3892   1885    914    849      2]
 [  1080  11218  38696  27590  14043   6530   3580   1957   2036     57]
 [   498   3818  14196  18103  15897  10151   5520   3109   4099    151]
 [   303   1947   5893   9447  11038  10244   6883   4345   6678    370]
 [   143    991   2823   4474   6455   7546   6764   4945   8670    649]
 [   154    629   1528   2647   3679   4595   5365   4575  10579    874]
 [   394   1539   2861   4800   6958   8326   9713  11392  53510  11703]
 [   146    489   1003   1617   2387   3179   4258   5059  37568  15982]]
2023-04-09 16:25:43,160 - INFO - Epoch: 4 | Validation Loss: 27.7123
2023-04-09 16:25:43,167 - INFO - Experiment ended. Checkpoints stored =)
